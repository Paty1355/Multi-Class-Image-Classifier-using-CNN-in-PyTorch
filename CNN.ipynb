{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e7dae3-c192-4267-a0ed-18d1ac56c861",
   "metadata": {
    "collapsed": true,
    "executionCancelledAt": null,
    "executionTime": 9213,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1740491984216,
    "lastExecutedByKernel": "a4535ae4-2239-4362-9b28-f73af3838ca6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "!pip install torchmetrics\n!pip install torchvision",
    "outputsMetadata": {
     "0": {
      "height": 544,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-1.5.2-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.23.2)\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (23.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.0)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
      "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (65.6.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.99)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->torchmetrics) (0.38.4)\n",
      "Downloading torchmetrics-1.5.2-py3-none-any.whl (891 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m891.4/891.4 kB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: lightning-utilities, torchmetrics\n",
      "Successfully installed lightning-utilities-0.11.9 torchmetrics-1.5.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchvision) (4.12.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.23.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==1.13.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.13.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.0->torchvision) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.0->torchvision) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.0->torchvision) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.0->torchvision) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchvision) (65.6.3)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchvision) (0.38.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->torchvision) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision) (2019.11.28)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea8065b7-84fc-4376-afef-6db731dec4b3",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 6998,
    "lastExecutedAt": 1740491991217,
    "lastExecutedByKernel": "a4535ae4-2239-4362-9b28-f73af3838ca6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchmetrics import Accuracy, Precision, Recall"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "662e1bf1-943f-4243-9fd4-02ce11609e8d",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 6067,
    "lastExecutedAt": 1740491997286,
    "lastExecutedByKernel": "a4535ae4-2239-4362-9b28-f73af3838ca6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Load datasets\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\n\ntrain_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\ntest_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())",
    "outputsMetadata": {
     "0": {
      "height": 80,
      "type": "stream"
     },
     "1": {
      "height": 38,
      "type": "stream"
     },
     "2": {
      "height": 122,
      "type": "stream"
     },
     "3": {
      "height": 38,
      "type": "stream"
     },
     "4": {
      "height": 122,
      "type": "stream"
     },
     "5": {
      "height": 38,
      "type": "stream"
     },
     "6": {
      "height": 122,
      "type": "stream"
     },
     "7": {
      "height": 38,
      "type": "stream"
     },
     "8": {
      "height": 59,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007555961608886719,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 26421880,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f4a09f511342bca78ee23e64271a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007479429244995117,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 29515,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a13c5c849b3491fa590a197dd5bddf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006454944610595703,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 4422102,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f65d595d9d4c2a8726aa5bbf331ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0066318511962890625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5148,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db577cc5123a4c54bfa4efccfa2bd7c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28fcb49-e49a-4350-ac61-552080a3ed68",
   "metadata": {},
   "source": [
    "Number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fb727aa-57ec-43b7-83fd-a246b2ca44ab",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1740493518486,
    "lastExecutedByKernel": "a4535ae4-2239-4362-9b28-f73af3838ca6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "classes = train_data.classes\nnum_classes = len(train_data.classes)\n\nnum_input_channels = 1\nnum_output_channels = 16\nimage_size = train_data[0][0].shape[1]"
   },
   "outputs": [],
   "source": [
    "classes = train_data.classes\n",
    "num_classes = len(train_data.classes)\n",
    "\n",
    "num_input_channels = 1\n",
    "num_output_channels = 16\n",
    "image_size = train_data[0][0].shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb09cd-b186-409e-9483-b9b9ee06c667",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53c0a71d-d7d9-4a11-8a9b-55867ea7e0b5",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1740493537322,
    "lastExecutedByKernel": "a4535ae4-2239-4362-9b28-f73af3838ca6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "class MultiClassImageClassifier(nn.Module):\n    def __init__(self, num_classes):\n        super(MultiClassImageClassifier, self).__init__()\n        self.conv1 = nn.Conv2d(num_input_channels, num_output_channels, kernel_size=3, stride=1, padding=1)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.flatten = nn.Flatten()\n\n        self.fc = nn.Linear(num_output_channels * (image_size//2)**2, num_classes)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.flatten(x)\n        x = self.fc(x)\n        return x"
   },
   "outputs": [],
   "source": [
    "class MultiClassImageClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultiClassImageClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_input_channels, num_output_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc = nn.Linear(num_output_channels * (image_size//2)**2, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dfd24f-58aa-4ed1-8617-ab5cd1a66cef",
   "metadata": {},
   "source": [
    "Training DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "065cf275-68f2-406b-b23a-ce2081fae28e",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 9,
    "lastExecutedAt": 1740493549135,
    "lastExecutedByKernel": "a4535ae4-2239-4362-9b28-f73af3838ca6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "dataloader_train = DataLoader(\n    train_data,\n    batch_size=10,\n    shuffle=True,\n)"
   },
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa8fc36-472c-47aa-9dc4-6c137a67011d",
   "metadata": {},
   "source": [
    "Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7cd76b8-d93a-41d7-b2d0-848ce9f934ed",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 10,
    "lastExecutedAt": 1740493561757,
    "lastExecutedByKernel": "a4535ae4-2239-4362-9b28-f73af3838ca6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def train_model(optimizer, net, num_epochs):\n    num_processed = 0\n    criterion = nn.CrossEntropyLoss()\n    for epoch in range(num_epochs):\n        running_loss = 0\n        num_processed = 0\n        for features, labels in dataloader_train:\n            optimizer.zero_grad()\n            outputs = net(features)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n            num_processed += len(labels)\n        print(f'epoch {epoch}, loss: {running_loss / num_processed}')\n        \n    train_loss = running_loss / len(dataloader_train)"
   },
   "outputs": [],
   "source": [
    "def train_model(optimizer, net, num_epochs):\n",
    "    num_processed = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        num_processed = 0\n",
    "        for features, labels in dataloader_train:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            num_processed += len(labels)\n",
    "        print(f'epoch {epoch}, loss: {running_loss / num_processed}')\n",
    "        \n",
    "    train_loss = running_loss / len(dataloader_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e49a5f-a515-475e-812c-7021a52fa397",
   "metadata": {},
   "source": [
    "Training for 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13e43672-c38c-43aa-a059-da6dc27bb2d0",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1740493582750,
    "lastExecutedByKernel": "a4535ae4-2239-4362-9b28-f73af3838ca6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "net = MultiClassImageClassifier(num_classes)\noptimizer = optim.Adam(net.parameters(), lr=0.001)"
   },
   "outputs": [],
   "source": [
    "net = MultiClassImageClassifier(num_classes)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98f47646-49a2-455e-9db5-29f4fd0ac225",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 57715,
    "lastExecutedAt": 1740493659094,
    "lastExecutedByKernel": "a4535ae4-2239-4362-9b28-f73af3838ca6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "train_model(\n    optimizer=optimizer,\n    net=net,\n    num_epochs=1,\n)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 0.03898408574059916\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    optimizer=optimizer,\n",
    "    net=net,\n",
    "    num_epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf45592-01e9-423f-9a8b-6bf8b21a2c29",
   "metadata": {},
   "source": [
    "Test DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33853c4a-beeb-4001-8009-b92a3adaa9f7",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 9,
    "lastExecutedAt": 1740493705658,
    "lastExecutedByKernel": "a4535ae4-2239-4362-9b28-f73af3838ca6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "dataloader_test = DataLoader(\n    test_data,\n    batch_size = 10,\n    shuffle = False\n)"
   },
   "outputs": [],
   "source": [
    "dataloader_test = DataLoader(\n",
    "    test_data,\n",
    "    batch_size = 10,\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160ab8c2-8b8f-4223-99b1-c8c7c44516d5",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81db8181-9557-4e20-8164-eb02b691b789",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 14,
    "lastExecutedAt": 1740493818467,
    "lastExecutedByKernel": "a4535ae4-2239-4362-9b28-f73af3838ca6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "accuracy_m = Accuracy(task='multiclass', num_classes=num_classes)\nprecision_m = Precision(task='multiclass', num_classes=num_classes, average=None)\nrecall_m = Recall(task='multiclass', num_classes=num_classes, average=None)"
   },
   "outputs": [],
   "source": [
    "accuracy_m = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "precision_m = Precision(task='multiclass', num_classes=num_classes, average=None)\n",
    "recall_m = Recall(task='multiclass', num_classes=num_classes, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6d00c1-ed1d-47ba-bd53-1c58bc6fd131",
   "metadata": {},
   "source": [
    "Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a23c170-3376-4a66-a73e-4a148132abde",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 11248,
    "lastExecutedAt": 1740494029640,
    "lastExecutedByKernel": "a4535ae4-2239-4362-9b28-f73af3838ca6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "net.eval()\npredictions = []\n\nfor i, (features,labels) in enumerate(dataloader_test):\n    output = net.forward(features.reshape(-1,1, image_size, image_size))\n    cat = torch.argmax(output, dim=1)\n    predictions.extend(cat.tolist())\n    accuracy_m(cat, labels)\n    precision_m(cat, labels)\n    recall_m(cat, labels)\n    \naccuracy = accuracy_m.compute().item()\nprecision = precision_m.compute().tolist()\nrecall = recall_m.compute().tolist()\n\nprint('Accuracy:', accuracy)\nprint('Precision (per class):', precision)\nprint('Recall (per class):', recall)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8881000280380249\n",
      "Precision (per class): [0.8095238208770752, 0.9808080792427063, 0.8508108258247375, 0.8881118893623352, 0.7862939834594727, 0.9876288771629333, 0.7087486386299133, 0.9342995285987854, 0.9749498963356018, 0.9589178562164307]\n",
      "Recall (per class): [0.8669999837875366, 0.9710000157356262, 0.7870000004768372, 0.8889999985694885, 0.871999979019165, 0.9580000042915344, 0.6399999856948853, 0.9670000076293945, 0.9729999899864197, 0.9570000171661377]\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "predictions = []\n",
    "\n",
    "for i, (features,labels) in enumerate(dataloader_test):\n",
    "    output = net.forward(features.reshape(-1,1, image_size, image_size))\n",
    "    cat = torch.argmax(output, dim=1)\n",
    "    predictions.extend(cat.tolist())\n",
    "    accuracy_m(cat, labels)\n",
    "    precision_m(cat, labels)\n",
    "    recall_m(cat, labels)\n",
    "    \n",
    "accuracy = accuracy_m.compute().item()\n",
    "precision = precision_m.compute().tolist()\n",
    "recall = recall_m.compute().tolist()\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision (per class):', precision)\n",
    "print('Recall (per class):', recall)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "editor": "DataLab",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
