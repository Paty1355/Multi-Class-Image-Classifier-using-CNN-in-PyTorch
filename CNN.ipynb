{"cells":[{"source":"![clothing_classification](clothing_classification.png)\n","metadata":{},"id":"aaa02648-9eae-45ba-893f-88440e8e4235","cell_type":"markdown"},{"source":"Fashion Forward is a new AI-based e-commerce clothing retailer.\nThey want to use image classification to automatically categorize new product listings, making it easier for customers to find what they're looking for. It will also assist in inventory management by quickly sorting items.\n\nAs a data scientist tasked with implementing a garment classifier, your primary objective is to develop a machine learning model capable of accurately categorizing images of clothing items into distinct garment types such as shirts, trousers, shoes, etc.","metadata":{},"id":"ad5a988c-1095-485a-a88c-002400a872be","cell_type":"markdown"},{"source":"# Run the cells below first","metadata":{"executionCancelledAt":null,"executionTime":872,"lastExecutedAt":1713283114116,"lastExecutedByKernel":"ab12a397-c392-4a15-94ff-20186a93f541","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Run the cells below first"},"id":"4a1ab317-f3e4-4e5f-93a7-9c27677c5ffb","cell_type":"code","execution_count":1,"outputs":[]},{"source":"!pip install torchmetrics\n!pip install torchvision","metadata":{"executionCancelledAt":null,"executionTime":9213,"lastExecutedAt":1740491984216,"lastExecutedByKernel":"a4535ae4-2239-4362-9b28-f73af3838ca6","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install torchmetrics\n!pip install torchvision","outputsMetadata":{"0":{"height":544,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"id":"93e7dae3-c192-4267-a0ed-18d1ac56c861","cell_type":"code","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nCollecting torchmetrics\n  Downloading torchmetrics-1.5.2-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.23.2)\nRequirement already satisfied: packaging>17.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (23.2)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.0)\nCollecting lightning-utilities>=0.8.0 (from torchmetrics)\n  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.12.2)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (65.6.3)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.99)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (8.5.0.96)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.10.3.66)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.99)\nRequirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->torchmetrics) (0.38.4)\nDownloading torchmetrics-1.5.2-py3-none-any.whl (891 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m891.4/891.4 kB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\nInstalling collected packages: lightning-utilities, torchmetrics\nSuccessfully installed lightning-utilities-0.11.9 torchmetrics-1.5.2\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchvision) (4.12.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.23.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.31.0)\nRequirement already satisfied: torch==1.13.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.13.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (9.2.0)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.0->torchvision) (11.7.99)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.0->torchvision) (8.5.0.96)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.0->torchvision) (11.10.3.66)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.0->torchvision) (11.7.99)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchvision) (65.6.3)\nRequirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchvision) (0.38.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision) (2.8)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->torchvision) (1.25.8)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision) (2019.11.28)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"}]},{"source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchmetrics import Accuracy, Precision, Recall","metadata":{"executionCancelledAt":null,"executionTime":6998,"lastExecutedAt":1740491991217,"lastExecutedByKernel":"a4535ae4-2239-4362-9b28-f73af3838ca6","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchmetrics import Accuracy, Precision, Recall"},"id":"ea8065b7-84fc-4376-afef-6db731dec4b3","cell_type":"code","execution_count":2,"outputs":[]},{"source":"# Load datasets\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\n\ntrain_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\ntest_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())","metadata":{"executionCancelledAt":null,"executionTime":6067,"lastExecutedAt":1740491997286,"lastExecutedByKernel":"a4535ae4-2239-4362-9b28-f73af3838ca6","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Load datasets\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\n\ntrain_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\ntest_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())","outputsMetadata":{"0":{"height":80,"type":"stream"},"1":{"height":38,"type":"stream"},"2":{"height":122,"type":"stream"},"3":{"height":38,"type":"stream"},"4":{"height":122,"type":"stream"},"5":{"height":38,"type":"stream"},"6":{"height":122,"type":"stream"},"7":{"height":38,"type":"stream"},"8":{"height":59,"type":"stream"}}},"id":"662e1bf1-943f-4243-9fd4-02ce11609e8d","cell_type":"code","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":"Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/26421880 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44f4a09f511342bca78ee23e64271a87"},"application/json":{"n":0,"total":26421880,"elapsed":0.007555961608886719,"ncols":null,"nrows":null,"prefix":"","ascii":false,"unit":"it","unit_scale":false,"rate":null,"bar_format":null,"postfix":null,"unit_divisor":1000,"initial":0,"colour":null}},"metadata":{}},{"output_type":"stream","name":"stdout","text":"Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/29515 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a13c5c849b3491fa590a197dd5bddf8"},"application/json":{"n":0,"total":29515,"elapsed":0.007479429244995117,"ncols":null,"nrows":null,"prefix":"","ascii":false,"unit":"it","unit_scale":false,"rate":null,"bar_format":null,"postfix":null,"unit_divisor":1000,"initial":0,"colour":null}},"metadata":{}},{"output_type":"stream","name":"stdout","text":"Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4422102 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15f65d595d9d4c2a8726aa5bbf331ffe"},"application/json":{"n":0,"total":4422102,"elapsed":0.006454944610595703,"ncols":null,"nrows":null,"prefix":"","ascii":false,"unit":"it","unit_scale":false,"rate":null,"bar_format":null,"postfix":null,"unit_divisor":1000,"initial":0,"colour":null}},"metadata":{}},{"output_type":"stream","name":"stdout","text":"Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5148 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db577cc5123a4c54bfa4efccfa2bd7c7"},"application/json":{"n":0,"total":5148,"elapsed":0.0066318511962890625,"ncols":null,"nrows":null,"prefix":"","ascii":false,"unit":"it","unit_scale":false,"rate":null,"bar_format":null,"postfix":null,"unit_divisor":1000,"initial":0,"colour":null}},"metadata":{}},{"output_type":"stream","name":"stdout","text":"Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n\n"}]},{"source":"Number of classes","metadata":{},"cell_type":"markdown","id":"d28fcb49-e49a-4350-ac61-552080a3ed68"},{"source":"classes = train_data.classes\nnum_classes = len(train_data.classes)\n\nnum_input_channels = 1\nnum_output_channels = 16\nimage_size = train_data[0][0].shape[1]","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1740493518486,"lastExecutedByKernel":"a4535ae4-2239-4362-9b28-f73af3838ca6","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"classes = train_data.classes\nnum_classes = len(train_data.classes)\n\nnum_input_channels = 1\nnum_output_channels = 16\nimage_size = train_data[0][0].shape[1]"},"cell_type":"code","id":"1fb727aa-57ec-43b7-83fd-a246b2ca44ab","outputs":[],"execution_count":21},{"source":"CNN","metadata":{},"cell_type":"markdown","id":"21eb09cd-b186-409e-9483-b9b9ee06c667"},{"source":"class MultiClassImageClassifier(nn.Module):\n    def __init__(self, num_classes):\n        super(MultiClassImageClassifier, self).__init__()\n        self.conv1 = nn.Conv2d(num_input_channels, num_output_channels, kernel_size=3, stride=1, padding=1)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.flatten = nn.Flatten()\n\n        self.fc = nn.Linear(num_output_channels * (image_size//2)**2, num_classes)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.flatten(x)\n        x = self.fc(x)\n        return x","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1740493537322,"lastExecutedByKernel":"a4535ae4-2239-4362-9b28-f73af3838ca6","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"class MultiClassImageClassifier(nn.Module):\n    def __init__(self, num_classes):\n        super(MultiClassImageClassifier, self).__init__()\n        self.conv1 = nn.Conv2d(num_input_channels, num_output_channels, kernel_size=3, stride=1, padding=1)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.flatten = nn.Flatten()\n\n        self.fc = nn.Linear(num_output_channels * (image_size//2)**2, num_classes)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.flatten(x)\n        x = self.fc(x)\n        return x"},"id":"53c0a71d-d7d9-4a11-8a9b-55867ea7e0b5","cell_type":"code","execution_count":22,"outputs":[]},{"source":"Training DataLoader","metadata":{},"cell_type":"markdown","id":"f3dfd24f-58aa-4ed1-8617-ab5cd1a66cef"},{"source":"dataloader_train = DataLoader(\n    train_data,\n    batch_size=10,\n    shuffle=True,\n)","metadata":{"executionCancelledAt":null,"executionTime":9,"lastExecutedAt":1740493549135,"lastExecutedByKernel":"a4535ae4-2239-4362-9b28-f73af3838ca6","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"dataloader_train = DataLoader(\n    train_data,\n    batch_size=10,\n    shuffle=True,\n)"},"cell_type":"code","id":"065cf275-68f2-406b-b23a-ce2081fae28e","outputs":[],"execution_count":23},{"source":"Training Function","metadata":{},"cell_type":"markdown","id":"dfa8fc36-472c-47aa-9dc4-6c137a67011d"},{"source":"def train_model(optimizer, net, num_epochs):\n    num_processed = 0\n    criterion = nn.CrossEntropyLoss()\n    for epoch in range(num_epochs):\n        running_loss = 0\n        num_processed = 0\n        for features, labels in dataloader_train:\n            optimizer.zero_grad()\n            outputs = net(features)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n            num_processed += len(labels)\n        print(f'epoch {epoch}, loss: {running_loss / num_processed}')\n        \n    train_loss = running_loss / len(dataloader_train)","metadata":{"executionCancelledAt":null,"executionTime":10,"lastExecutedAt":1740493561757,"lastExecutedByKernel":"a4535ae4-2239-4362-9b28-f73af3838ca6","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def train_model(optimizer, net, num_epochs):\n    num_processed = 0\n    criterion = nn.CrossEntropyLoss()\n    for epoch in range(num_epochs):\n        running_loss = 0\n        num_processed = 0\n        for features, labels in dataloader_train:\n            optimizer.zero_grad()\n            outputs = net(features)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n            num_processed += len(labels)\n        print(f'epoch {epoch}, loss: {running_loss / num_processed}')\n        \n    train_loss = running_loss / len(dataloader_train)"},"cell_type":"code","id":"c7cd76b8-d93a-41d7-b2d0-848ce9f934ed","outputs":[],"execution_count":24},{"source":"Training for 1 epoch","metadata":{},"cell_type":"markdown","id":"33e49a5f-a515-475e-812c-7021a52fa397"},{"source":"net = MultiClassImageClassifier(num_classes)\noptimizer = optim.Adam(net.parameters(), lr=0.001)","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1740493582750,"lastExecutedByKernel":"a4535ae4-2239-4362-9b28-f73af3838ca6","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"net = MultiClassImageClassifier(num_classes)\noptimizer = optim.Adam(net.parameters(), lr=0.001)"},"cell_type":"code","id":"13e43672-c38c-43aa-a059-da6dc27bb2d0","outputs":[],"execution_count":25},{"source":"train_model(\n    optimizer=optimizer,\n    net=net,\n    num_epochs=1,\n)","metadata":{"executionCancelledAt":null,"executionTime":57715,"lastExecutedAt":1740493659094,"lastExecutedByKernel":"a4535ae4-2239-4362-9b28-f73af3838ca6","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"train_model(\n    optimizer=optimizer,\n    net=net,\n    num_epochs=1,\n)"},"cell_type":"code","id":"98f47646-49a2-455e-9db5-29f4fd0ac225","outputs":[{"output_type":"stream","name":"stdout","text":"epoch 0, loss: 0.03898408574059916\n"}],"execution_count":26},{"source":"Test DataLoader","metadata":{},"cell_type":"markdown","id":"eaf45592-01e9-423f-9a8b-6bf8b21a2c29"},{"source":"dataloader_test = DataLoader(\n    test_data,\n    batch_size = 10,\n    shuffle = False\n)","metadata":{"executionCancelledAt":null,"executionTime":9,"lastExecutedAt":1740493705658,"lastExecutedByKernel":"a4535ae4-2239-4362-9b28-f73af3838ca6","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"dataloader_test = DataLoader(\n    test_data,\n    batch_size = 10,\n    shuffle = False\n)"},"cell_type":"code","id":"33853c4a-beeb-4001-8009-b92a3adaa9f7","outputs":[],"execution_count":27},{"source":"Metrics","metadata":{},"cell_type":"markdown","id":"160ab8c2-8b8f-4223-99b1-c8c7c44516d5"},{"source":"accuracy_m = Accuracy(task='multiclass', num_classes=num_classes)\nprecision_m = Precision(task='multiclass', num_classes=num_classes, average=None)\nrecall_m = Recall(task='multiclass', num_classes=num_classes, average=None)","metadata":{"executionCancelledAt":null,"executionTime":14,"lastExecutedAt":1740493818467,"lastExecutedByKernel":"a4535ae4-2239-4362-9b28-f73af3838ca6","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"accuracy_m = Accuracy(task='multiclass', num_classes=num_classes)\nprecision_m = Precision(task='multiclass', num_classes=num_classes, average=None)\nrecall_m = Recall(task='multiclass', num_classes=num_classes, average=None)"},"cell_type":"code","id":"81db8181-9557-4e20-8164-eb02b691b789","outputs":[],"execution_count":28},{"source":"Compute metrics","metadata":{},"cell_type":"markdown","id":"7d6d00c1-ed1d-47ba-bd53-1c58bc6fd131"},{"source":"net.eval()\npredictions = []\n\nfor i, (features,labels) in enumerate(dataloader_test):\n    output = net.forward(features.reshape(-1,1, image_size, image_size))\n    cat = torch.argmax(output, dim=1)\n    predictions.extend(cat.tolist())\n    accuracy_m(cat, labels)\n    precision_m(cat, labels)\n    recall_m(cat, labels)\n    \naccuracy = accuracy_m.compute().item()\nprecision = precision_m.compute().tolist()\nrecall = recall_m.compute().tolist()\n\nprint('Accuracy:', accuracy)\nprint('Precision (per class):', precision)\nprint('Recall (per class):', recall)","metadata":{"executionCancelledAt":null,"executionTime":11248,"lastExecutedAt":1740494029640,"lastExecutedByKernel":"a4535ae4-2239-4362-9b28-f73af3838ca6","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"net.eval()\npredictions = []\n\nfor i, (features,labels) in enumerate(dataloader_test):\n    output = net.forward(features.reshape(-1,1, image_size, image_size))\n    cat = torch.argmax(output, dim=1)\n    predictions.extend(cat.tolist())\n    accuracy_m(cat, labels)\n    precision_m(cat, labels)\n    recall_m(cat, labels)\n    \naccuracy = accuracy_m.compute().item()\nprecision = precision_m.compute().tolist()\nrecall = recall_m.compute().tolist()\n\nprint('Accuracy:', accuracy)\nprint('Precision (per class):', precision)\nprint('Recall (per class):', recall)"},"cell_type":"code","id":"7a23c170-3376-4a66-a73e-4a148132abde","outputs":[{"output_type":"stream","name":"stdout","text":"Accuracy: 0.8881000280380249\nPrecision (per class): [0.8095238208770752, 0.9808080792427063, 0.8508108258247375, 0.8881118893623352, 0.7862939834594727, 0.9876288771629333, 0.7087486386299133, 0.9342995285987854, 0.9749498963356018, 0.9589178562164307]\nRecall (per class): [0.8669999837875366, 0.9710000157356262, 0.7870000004768372, 0.8889999985694885, 0.871999979019165, 0.9580000042915344, 0.6399999856948853, 0.9670000076293945, 0.9729999899864197, 0.9570000171661377]\n"}],"execution_count":29}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}